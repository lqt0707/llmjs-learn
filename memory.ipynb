{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7e0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatMessageHistory} from 'langchain/stores/message/in_memory'\n",
    "import {HumanMessage,AIMessage} from '@langchain/core/messages'\n",
    "\n",
    "const history = new ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96743b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.addMessage(new HumanMessage('hi'))\n",
    "await history.addMessage(new AIMessage('what can I do for you?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd7ba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  HumanMessage {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: { content: \"hi\", additional_kwargs: {}, response_metadata: {} },\n",
      "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "    content: \"hi\",\n",
      "    name: undefined,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: {\n",
      "      content: \"what can I do for you?\",\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    lc_namespace: [ \"langchain_core\", \"messages\" ],\n",
      "    content: \"what can I do for you?\",\n",
      "    name: undefined,\n",
      "    additional_kwargs: {},\n",
      "    response_metadata: {}\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const messages = await history.getMessages()\n",
    "\n",
    "console.log(messages);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8675216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {  Ollama } from \"npm:/@langchain/ollama@0.1.5\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "\n",
    "const chatModel = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\",\n",
    "  model: \"qwen2.5:0.5b\",\n",
    "});\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "  You are talkative and provides lots of specific details from its context. \n",
    "  If the you does not know the answer to a question, it truthfully says you do not know.`,\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "]);\n",
    "\n",
    "const chain = prompt.pipe(chatModel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb9acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addMessage(new HumanMessage(\"hi, my name is Kai\"));\n",
    "\n",
    "const res1 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff62370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Hello Kai! It's nice to meet you. I'm here to provide information about your system and any relevant details for you. Please feel free to ask me anything you'd like to know.\"\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6ebe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.addMessage(new AIMessage(res1))\n",
    "await history.addMessage(new HumanMessage('what is my name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bce0f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc45a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Your name is \"Kai\".'\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a8410",
   "metadata": {},
   "source": [
    "##自动维护chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0e59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Ollama } from \"npm:/@langchain/ollama@0.1.5\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "\n",
    "// const chatModel = new Ollama({\n",
    "//   baseUrl: \"http://localhost:11434\",\n",
    "//   model: \"qwen2.5:0.5b\",\n",
    "// });\n",
    "const chatModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.moonshot.cn/v1\",\n",
    "    apiKey: process.env.OPENAI_API_KEY,\n",
    "  },\n",
    "  modelName: \"kimi-k2-0711-preview\",\n",
    "});\n",
    "\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "  [\"human\", \"{input}\"],\n",
    "]);\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "const chain = prompt.pipe(chatModel);\n",
    "\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: chain,\n",
    "  getMessageHistory: (_sessionId) => history,\n",
    "  inputMessagesKey: \"input\",\n",
    "  historyMessagesKey: \"history_message\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ccaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "const res1 = await chainWithHistory.invoke(\n",
    "  { input: \"hi,my name is Kai\" },\n",
    "  { configurable: { sessionId: \"none\" } }\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74237c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb906af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chainWithHistory.invoke(\n",
    "  { input: \"我的名字叫什么\" },\n",
    "  { configurable: { sessionId: \"none\" } }\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1adc8439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"你的名字是 **Kai**。\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "    response_metadata: {}\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "  content: \u001b[32m\"你的名字是 **Kai**。\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m },\n",
       "  response_metadata: {\n",
       "    tokenUsage: { completionTokens: \u001b[33m7\u001b[39m, promptTokens: \u001b[33m58\u001b[39m, totalTokens: \u001b[33m65\u001b[39m },\n",
       "    finish_reason: \u001b[32m\"stop\"\u001b[39m\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.getMessages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd0dac",
   "metadata": {},
   "source": [
    "自动生成chat history摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a38b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\";\n",
    "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5885f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "const summaryModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.moonshot.cn/v1\",\n",
    "    apiKey: process.env.OPENAI_API_KEY,\n",
    "  },\n",
    "  modelName: \"kimi-k2-0711-preview\",\n",
    "});\n",
    "\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "  Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "  \n",
    "  Current summary:\n",
    "  {summary}\n",
    "  \n",
    "  New lines of conversation:\n",
    "  {new_lines}\n",
    "  \n",
    "  New summary:\n",
    "  `); \n",
    "\n",
    "  const summaryChain = RunnableSequence.from([\n",
    "    summaryPrompt,\n",
    "    summaryModel,\n",
    "    new StringOutputParser(),\n",
    "  ]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa6052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "const newSummary = await summaryChain.invoke({\n",
    "  summary: \"\",\n",
    "  new_lines: \"I'm 18\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729c95e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"The speaker is an 18-year-old male.\"\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await summaryChain.invoke({\n",
    "  summary: newSummary,\n",
    "  new_lines: \"I'm male\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13f027a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\";\n",
    "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n",
    "\n",
    "const summaryModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.moonshot.cn/v1\",\n",
    "    apiKey: process.env.OPENAI_API_KEY,\n",
    "  },\n",
    "  modelName: \"kimi-k2-0711-preview\",\n",
    "});\n",
    "\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "  Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "  \n",
    "  Current summary:\n",
    "  {summary}\n",
    "  \n",
    "  New lines of conversation:\n",
    "  {new_lines}\n",
    "  \n",
    "  New summary:\n",
    "  `);\n",
    "\n",
    "const summaryChain = RunnableSequence.from([\n",
    "  summaryPrompt,\n",
    "  summaryModel,\n",
    "  new StringOutputParser(),\n",
    "]);\n",
    "\n",
    "const chatModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.moonshot.cn/v1\",\n",
    "    apiKey: process.env.OPENAI_API_KEY,\n",
    "  },\n",
    "  modelName: \"kimi-k2-0711-preview\",\n",
    "});\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "  Here is the chat history summary:\n",
    "  {history_summary}\n",
    "  `,\n",
    "  ],\n",
    "  [\"human\", \"{input}\"],\n",
    "]);\n",
    "\n",
    "let summary = \"\";\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "const chatChain = RunnableSequence.from([\n",
    "  {\n",
    "    input: new RunnablePassthrough({\n",
    "      func: (input) => history.addUserMessage(input),\n",
    "    }),\n",
    "  },\n",
    "  RunnablePassthrough.assign({\n",
    "    history_summary: () => summary,\n",
    "  }),\n",
    "  chatPrompt,\n",
    "  chatModel,\n",
    "  new StringOutputParser(),\n",
    "  new RunnablePassthrough({\n",
    "    func: async (input) => {\n",
    "      history.addAIMessage(input);\n",
    "      const messages = await history.getMessages();\n",
    "      const new_lines = getBufferString(messages);\n",
    "      const newSummary = await summaryChain.invoke({\n",
    "        summary,\n",
    "        new_lines,\n",
    "      });\n",
    "      history.clear();\n",
    "      summary = newSummary;\n",
    "    },\n",
    "  }),\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8adc385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"先帮你解决当下饥饿！给你三套“5-10 分钟能搞定”的方案，按手边食材挑一个：\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"1. 极速碳水版 – 葱香煎蛋面  \\n\"\u001b[39m +\n",
       "  \u001b[32m\"   一把挂面 + 2 个鸡蛋 + 葱花 + 生抽/盐/香油。煮面时顺带煎个荷包蛋，面捞起加汤或直接拌酱，撒葱花完成。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"2. 便利店极简版 – 饭团+配菜  \\n\"\u001b[39m +\n",
       "  \u001b[32m\"   去最近便利店买一只金枪鱼蛋黄酱饭团 + 一份即食毛豆/溏心蛋，边走边吃顶饿又抗饿。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"3. 零烹饪版 – 冰箱拼盘  \\n\"\u001b[39m +\n",
       "  \u001b[32m\"   冰箱里如有吐司/馒头、花生酱/辣酱、番茄/黄瓜，任意组合厚涂酱+切片蔬菜夹着吃，再来一杯牛奶。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"没有上述材料？告诉我你家里还剩啥，我帮你 1:1 现场配菜谱！\"\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chatChain.invoke('我饿了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f34ca043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"好！既然今天特别想吃方便面，那就把“速食”做出升级版的幸福感，给你 3 条 5 分钟路线，任选或自由混搭。你只告诉我现在手头有啥——冰箱里剩下的蔬菜、肉类、罐头、酱料都算——我立刻帮你“私人定制”。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"————————\\n\"\u001b[39m +\n",
       "  \u001b[32m\"1. 3 级进阶版\\n\"\u001b[39m +\n",
       "  \u001b[32m\"经典三件套：方便面饼＋自带调料 → 煎/煮一个溏心荷包蛋 → 撒葱花＋海苔碎＋白芝麻。关键是最后 10 秒淋半勺热油（可用微波炉热 15 秒代替），滋啦一声，香气翻倍。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"2. 不用碗的韩式芝士锅\\n\"\u001b[39m +\n",
       "  \u001b[32m\"① 小号不粘锅直接把面饼摆上；② 倒开水没过面，中火 1 分 30 秒后把泡面自带的粉包减半量洒进去；③ 翻进两片芝士（或一小块马苏里拉）＋泡菜/午餐肉丁；④ 汤汁收浓稠时关火，用筷子搅拌拉丝。全程不进碗，锅就是碗。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"3. 5 分钟“伪拉面”\\n\"\u001b[39m +\n",
       "  \u001b[32m\"把面煮至 6 分熟捞出（锅里水别倒）。  \\n\"\u001b[39m +\n",
       "  \u001b[32m\"替换高汤：原锅水＋味噌半勺＋生抽 1 小勺＋香油几滴。  \\n\"\u001b[39m +\n",
       "  \u001b[32m\"直接把面回锅＋打开任意速冻丸子 4–5 粒，小青菜一把，瞬间拥有“拉面店同款”味儿。\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"————————\\n\"\u001b[39m +\n",
       "  \u001b[32m\"如果遇到以下情况，直接告诉我，我 30 秒给你改方案：\\n\"\u001b[39m +\n",
       "  \u001b[32m\"- 没青菜/没蛋/没芝士  \\n\"\u001b[39m +\n",
       "  \u001b[32m\"- 想让方便面少油少盐  \\n\"\u001b[39m +\n",
       "  \u001b[32m\"- 多了一张吃剩的披萨皮/半袋速冻毛豆/任何冰箱边角料\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"你现在有的配料？\"\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chatChain.invoke('我今天想吃方便面')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
