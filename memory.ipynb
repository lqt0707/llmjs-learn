{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatMessageHistory} from 'langchain/stores/message/in_memory'\n",
    "import {HumanMessage,AIMessage} from '@langchain/core/messages'\n",
    "\n",
    "const history = new ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96743b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.addMessage(new HumanMessage('hi'))\n",
    "await history.addMessage(new AIMessage('what can I do for you?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "const messages = await history.getMessages()\n",
    "\n",
    "console.log(messages);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import {  Ollama } from \"npm:/@langchain/ollama@0.1.5\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "\n",
    "const chatModel = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\",\n",
    "  model: \"qwen2.5:0.5b\",\n",
    "});\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "  You are talkative and provides lots of specific details from its context. \n",
    "  If the you does not know the answer to a question, it truthfully says you do not know.`,\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "]);\n",
    "\n",
    "const chain = prompt.pipe(chatModel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb9acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "await history.addMessage(new HumanMessage(\"hi, my name is Kai\"));\n",
    "\n",
    "const res1 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff62370",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ebe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.addMessage(new AIMessage(res1))\n",
    "await history.addMessage(new HumanMessage('what is my name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chain.invoke({\n",
    "  history_message: await history.getMessages(),\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc45a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a8410",
   "metadata": {},
   "source": [
    "##自动维护chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { Ollama } from \"npm:/@langchain/ollama@0.1.5\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "\n",
    "// const chatModel = new Ollama({\n",
    "//   baseUrl: \"http://localhost:11434\",\n",
    "//   model: \"qwen2.5:0.5b\",\n",
    "// });\n",
    "const chatModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.moonshot.cn/v1\",\n",
    "    apiKey: process.env.OPENAI_API_KEY,\n",
    "  },\n",
    "  modelName: \"kimi-k2-0711-preview\",\n",
    "});\n",
    "\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "  ],\n",
    "  new MessagesPlaceholder(\"history_message\"),\n",
    "  [\"human\", \"{input}\"],\n",
    "]);\n",
    "\n",
    "const history = new ChatMessageHistory();\n",
    "const chain = prompt.pipe(chatModel);\n",
    "\n",
    "const chainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: chain,\n",
    "  getMessageHistory: (_sessionId) => history,\n",
    "  inputMessagesKey: \"input\",\n",
    "  historyMessagesKey: \"history_message\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ccaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "const res1 = await chainWithHistory.invoke(\n",
    "  { input: \"hi,my name is Kai\" },\n",
    "  { configurable: { sessionId: \"none\" } }\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74237c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb906af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "const res2 = await chainWithHistory.invoke(\n",
    "  { input: \"我的名字叫什么\" },\n",
    "  { configurable: { sessionId: \"none\" } }\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "await history.getMessages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd0dac",
   "metadata": {},
   "source": [
    "自动生成chat history摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\";\n",
    "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5885f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "const summaryModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.moonshot.cn/v1\",\n",
    "    apiKey: process.env.OPENAI_API_KEY,\n",
    "  },\n",
    "  modelName: \"kimi-k2-0711-preview\",\n",
    "});\n",
    "\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "  Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "  \n",
    "  Current summary:\n",
    "  {summary}\n",
    "  \n",
    "  New lines of conversation:\n",
    "  {new_lines}\n",
    "  \n",
    "  New summary:\n",
    "  `); \n",
    "\n",
    "  const summaryChain = RunnableSequence.from([\n",
    "    summaryPrompt,\n",
    "    summaryModel,\n",
    "    new StringOutputParser(),\n",
    "  ]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "const newSummary = await summaryChain.invoke({\n",
    "  summary: \"\",\n",
    "  new_lines: \"I'm 18\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "await summaryChain.invoke({\n",
    "  summary: newSummary,\n",
    "  new_lines: \"I'm male\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f027a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n",
    "import {\n",
    "  ChatPromptTemplate,\n",
    "  MessagesPlaceholder,\n",
    "} from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "import { RunnableSequence } from \"@langchain/core/runnables\";\n",
    "import { RunnablePassthrough } from \"@langchain/core/runnables\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "import { getBufferString } from \"@langchain/core/messages\";\n",
    "\n",
    "const summaryModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.moonshot.cn/v1\",\n",
    "    apiKey: process.env.OPENAI_API_KEY,\n",
    "  },\n",
    "  modelName: \"kimi-k2-0711-preview\",\n",
    "});\n",
    "\n",
    "const summaryPrompt = ChatPromptTemplate.fromTemplate(`\n",
    "  Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary\n",
    "  \n",
    "  Current summary:\n",
    "  {summary}\n",
    "  \n",
    "  New lines of conversation:\n",
    "  {new_lines}\n",
    "  \n",
    "  New summary:\n",
    "  `);\n",
    "\n",
    "const summaryChain = RunnableSequence.from([\n",
    "  summaryPrompt,\n",
    "  summaryModel,\n",
    "  new StringOutputParser(),\n",
    "]);\n",
    "\n",
    "const chatModel = new ChatOpenAI({\n",
    "  configuration: {\n",
    "    baseURL: \"https://api.moonshot.cn/v1\",\n",
    "    apiKey: process.env.OPENAI_API_KEY,\n",
    "  },\n",
    "  modelName: \"kimi-k2-0711-preview\",\n",
    "});\n",
    "const chatPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\n",
    "    \"system\",\n",
    "    `You are a helpful assistant. Answer all questions to the best of your ability.\n",
    "\n",
    "  Here is the chat history summary:\n",
    "  {history_summary}\n",
    "  `,\n",
    "  ],\n",
    "  [\"human\", \"{input}\"],\n",
    "]);\n",
    "\n",
    "let summary = \"\";\n",
    "const history = new ChatMessageHistory();\n",
    "\n",
    "const chatChain = RunnableSequence.from([\n",
    "  {\n",
    "    input: new RunnablePassthrough({\n",
    "      func: (input) => history.addUserMessage(input),\n",
    "    }),\n",
    "  },\n",
    "  RunnablePassthrough.assign({\n",
    "    history_summary: () => summary,\n",
    "  }),\n",
    "  chatPrompt,\n",
    "  chatModel,\n",
    "  new StringOutputParser(),\n",
    "  new RunnablePassthrough({\n",
    "    func: async (input) => {\n",
    "      history.addAIMessage(input);\n",
    "      const messages = await history.getMessages();\n",
    "      const new_lines = getBufferString(messages);\n",
    "      const newSummary = await summaryChain.invoke({\n",
    "        summary,\n",
    "        new_lines,\n",
    "      });\n",
    "      history.clear();\n",
    "      summary = newSummary;\n",
    "    },\n",
    "  }),\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adc385",
   "metadata": {},
   "outputs": [],
   "source": [
    "await chatChain.invoke('我饿了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ca043",
   "metadata": {},
   "outputs": [],
   "source": [
    "await chatChain.invoke('我今天想吃方便面')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
